---
# Source: ory/templates/psp.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-ory
  labels:
    release: release-name
    helm.sh/chart: ory-1.1.0
    app.kubernetes.io/name: release-name-ory
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
spec:
  allowPrivilegeEscalation: false
  privileged: false
  hostNetwork: false
  hostIPC: false
  hostPID: false
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  runAsUser:
    rule: 'MustRunAsNonRoot'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  volumes:
    - "secret"
---
# Source: ory/charts/oathkeeper/charts/oathkeeper-maester/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: oathkeeper-maester-account
  namespace:  default
---
# Source: ory/charts/oathkeeper/templates/cronjob.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ory-oathkeeper-keys-service-account
  namespace: default
---
# Source: ory/charts/oathkeeper/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-oathkeeper
  labels:
    app: oathkeeper
    app.kubernetes.io/name: oathkeeper
    helm.sh/chart: oathkeeper-0.23.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.38.25-beta.1"
    app.kubernetes.io/managed-by: Helm
---
# Source: ory/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-postgresql
  labels:
    app: postgresql
    chart: postgresql-11.1.26
    release: "release-name"
    heritage: "Helm"
type: Opaque
data:
  postgresql-password: "Y29tcGFzcw=="
---
# Source: ory/charts/hydra/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-hydra
  namespace: default
  labels:
    "app.kubernetes.io/name": "hydra"
    "app.kubernetes.io/instance": "release-name"
    "app.kubernetes.io/version": "v1.11.8"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "hydra-0.23.3"
data:
  "hydra.yaml": |

    log:
      leak_sensitive_values: false
      level: trace
    serve:
      admin:
        port: 4445
      public:
        port: 4444
      tls:
        allow_termination_from:
        - 10.0.0.0/8
        - 172.16.0.0/12
        - 192.168.0.0/16
    urls:
      self: {}
---
# Source: ory/charts/oathkeeper/charts/oathkeeper-maester/templates/dashboard-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ory-oathkeeper-maester-dashboard
  labels:
    grafana_dashboard: "1"
    app: monitoring-grafana
data:
  ory-oathkeeper-maester-dashboard.json: |-
    {
      "annotations": {
        "list": [
        {
          "builtIn": 1,
          "datasource": "-- Grafana --",
          "enable": true,
          "hide": true,
          "iconColor": "rgba(0, 211, 255, 1)",
          "name": "Annotations & Alerts",
          "type": "dashboard"
        }
        ]
      },
      "editable": false,
      "gnetId": null,
      "graphTooltip": 0,
      "links": [],
      "panels": [
      {
        "collapsed": false,
        "gridPos": {
          "h": 1,
          "w": 24,
          "x": 0,
          "y": 0
        },
        "id": 10,
        "panels": [],
        "title": "Application metrics",
        "type": "row"
      },
      {
        "aliasColors": {},
        "bars": false,
        "dashLength": 10,
        "dashes": false,
        "datasource": "Prometheus",
        "fill": 1,
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 1
        },
        "id": 6,
        "legend": {
          "avg": false,
          "current": false,
          "max": false,
          "min": false,
          "show": true,
          "total": false,
          "values": false
        },
        "lines": true,
        "linewidth": 1,
        "links": [],
        "nullPointMode": "null",
        "paceLength": 10,
        "percentage": false,
        "pointradius": 2,
        "points": false,
        "renderer": "flot",
        "seriesOverrides": [],
        "stack": false,
        "steppedLine": false,
        "targets": [
        {
          "expr": "go_goroutines{service=\"ory-oathkeeper-maester-metrics\"}",
          "format": "time_series",
          "intervalFactor": 1,
          "refId": "A",
          "legendFormat": "{{ pod }}"
        }
        ],
        "thresholds": [],
        "timeFrom": null,
        "timeRegions": [],
        "timeShift": null,
        "title": "Goroutines",
        "tooltip": {
          "shared": true,
          "sort": 0,
          "value_type": "individual"
        },
        "type": "graph",
        "xaxis": {
          "buckets": null,
          "mode": "time",
          "name": null,
          "show": true,
          "values": []
        },
        "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
        ],
        "yaxis": {
          "align": false,
          "alignLevel": null
        }
      },
      {
        "aliasColors": {},
        "bars": false,
        "dashLength": 10,
        "dashes": false,
        "datasource": "Prometheus",
        "fill": 1,
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 1
        },
        "id": 8,
        "legend": {
          "avg": false,
          "current": false,
          "max": false,
          "min": false,
          "show": true,
          "total": false,
          "values": false
        },
        "lines": true,
        "linewidth": 1,
        "links": [],
        "nullPointMode": "null",
        "paceLength": 10,
        "percentage": false,
        "pointradius": 2,
        "points": false,
        "renderer": "flot",
        "seriesOverrides": [],
        "stack": false,
        "steppedLine": false,
        "targets": [
        {
          "expr": "go_threads{service=\"ory-oathkeeper-maester-metrics\"}",
          "format": "time_series",
          "intervalFactor": 1,
          "refId": "A",
          "legendFormat": "{{ pod }}"
        }
        ],
        "thresholds": [],
        "timeFrom": null,
        "timeRegions": [],
        "timeShift": null,
        "title": "Go threads",
        "tooltip": {
          "shared": true,
          "sort": 0,
          "value_type": "individual"
        },
        "type": "graph",
        "xaxis": {
          "buckets": null,
          "mode": "time",
          "name": null,
          "show": true,
          "values": []
        },
        "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
        ],
        "yaxis": {
          "align": false,
          "alignLevel": null
        }
      },
      {
        "aliasColors": {},
        "bars": false,
        "dashLength": 10,
        "dashes": false,
        "datasource": "Prometheus",
        "fill": 1,
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 9
        },
        "id": 12,
        "legend": {
          "avg": false,
          "current": false,
          "max": false,
          "min": false,
          "show": true,
          "total": false,
          "values": false
        },
        "lines": true,
        "linewidth": 1,
        "links": [],
        "nullPointMode": "null",
        "paceLength": 10,
        "percentage": false,
        "pointradius": 2,
        "points": false,
        "renderer": "flot",
        "seriesOverrides": [],
        "stack": false,
        "steppedLine": false,
        "targets": [
        {
          "expr": "go_gc_duration_seconds{service=\"ory-oathkeeper-maester-metrics\"}",
          "format": "time_series",
          "intervalFactor": 1,
          "refId": "A",
          "legendFormat": "quantile: {{ quantile }} ({{ pod }})"
        }
        ],
        "thresholds": [],
        "timeFrom": null,
        "timeRegions": [],
        "timeShift": null,
        "title": "GC invocations durations",
        "tooltip": {
          "shared": true,
          "sort": 0,
          "value_type": "individual"
        },
        "type": "graph",
        "xaxis": {
          "buckets": null,
          "mode": "time",
          "name": null,
          "show": true,
          "values": []
        },
        "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
        ],
        "yaxis": {
          "align": false,
          "alignLevel": null
        }
      },
      {
        "aliasColors": {},
        "bars": false,
        "dashLength": 10,
        "dashes": false,
        "datasource": "Prometheus",
        "fill": 1,
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 9
        },
        "id": 16,
        "legend": {
          "avg": false,
          "current": false,
          "max": false,
          "min": false,
          "show": true,
          "total": false,
          "values": false
        },
        "lines": true,
        "linewidth": 1,
        "links": [],
        "nullPointMode": "null",
        "paceLength": 10,
        "percentage": false,
        "pointradius": 2,
        "points": false,
        "renderer": "flot",
        "seriesOverrides": [],
        "stack": false,
        "steppedLine": false,
        "targets": [
        {
          "expr": "go_memstats_heap_inuse_bytes{job=\"ory-oathkeeper-maester-metrics\"}",
          "format": "time_series",
          "intervalFactor": 2,
          "legendFormat": "Heap in use",
          "refId": "A"
        },
        {
          "expr": "go_memstats_stack_inuse_bytes{job=\"ory-oathkeeper-maester-metrics\"}",
          "format": "time_series",
          "intervalFactor": 2,
          "legendFormat": "Stack in use",
          "refId": "B"
        },
        {
          "expr": "container_memory_usage_bytes{container=~\"oathkeeper-maester|istio-proxy\", pod=~\"ory-oathkeeper-maester.*\"}",
          "format": "time_series",
          "intervalFactor": 2,
          "legendFormat": "{{ container }} (k8s)",
          "refId": "C"
        },
        {
          "expr": "sum(container_memory_usage_bytes{container=~\"oathkeeper-maester|istio-proxy\", pod=~\"ory-oathkeeper-maester.*\"})",
          "format": "time_series",
          "intervalFactor": 2,
          "legendFormat": "Total",
          "refId": "D"
        }
        ],
        "thresholds": [],
        "timeFrom": null,
        "timeRegions": [],
        "timeShift": null,
        "title": "Memory",
        "tooltip": {
          "shared": true,
          "sort": 0,
          "value_type": "individual"
        },
        "type": "graph",
        "xaxis": {
          "buckets": null,
          "mode": "time",
          "name": null,
          "show": true,
          "values": []
        },
        "yaxes": [
        {
          "format": "bytes",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
        ],
        "yaxis": {
          "align": false,
          "alignLevel": null
        }
      }
      ],
      "schemaVersion": 18,
      "style": "dark",
      "tags": [
        "auth",
        "kyma"
      ],
      "templating": {
        "list": []
      },
      "time": {
        "from": "now-1h",
        "to": "now"
      },
      "refresh": "10s",
      "timepicker": {
        "refresh_intervals": [
          "5s",
          "10s",
          "30s",
          "1m",
          "5m",
          "15m",
          "30m",
          "1h",
          "2h",
          "1d"
        ],
        "time_options": [
          "5m",
          "15m",
          "1h",
          "6h",
          "12h",
          "24h",
          "2d",
          "7d",
          "30d"
        ]
      },
      "timezone": "",
      "title": "Kyma / ORY / Oathkeeper-maester",
      "version": 1
    }
---
# Source: ory/charts/oathkeeper/templates/configmap-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-oathkeeper-config
  namespace: default
  labels:
    app: oathkeeper
    app.kubernetes.io/name: oathkeeper
    helm.sh/chart: oathkeeper-0.23.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.38.25-beta.1"
    app.kubernetes.io/managed-by: Helm
data:
  "config.yaml": |
    access_rules:
      repositories:
      - file:///etc/rules/access-rules.json
    authenticators:
      anonymous:
        config:
          subject: anonymous
        enabled: true
      cookie_session:
        config:
          check_session_url: https://session-store-host
          only:
          - sessionid
        enabled: false
      jwt:
        config:
          jwks_urls:
          - http://ory-hydra-public.kyma-system.svc.cluster.local:4444/.well-known/jwks.json
          scope_strategy: wildcard
        enabled: true
      noop:
        enabled: true
      oauth2_client_credentials:
        config:
          token_url: http://ory-hydra-public.kyma-system.svc.cluster.local:4444/oauth2/token
        enabled: true
      oauth2_introspection:
        config:
          introspection_url: http://ory-hydra-admin.kyma-system.svc.cluster.local:4445/oauth2/introspect
          scope_strategy: exact
        enabled: true
      unauthorized:
        enabled: true
    authorizers:
      allow:
        enabled: true
      deny:
        enabled: true
    log:
      level: trace
    mutators:
      cookie:
        config:
          cookies:
            processedWith: oathkeeper
        enabled: true
      header:
        config:
          headers:
            X-Server: oathkeeper
        enabled: true
      hydrator:
        config:
          api:
            url: http://compass-director.compass-system.svc.cluster.local:3000/tenant-mapping
        enabled: true
      id_token:
        config:
          issuer_url: https://oathkeeper.local.kyma.dev/
          jwks_url: file:///etc/secrets/jwks.json
          ttl: 60s
        enabled: true
      noop:
        enabled: true
    serve:
      api:
        port: 4456
      prometheus:
        port: 9000
      proxy:
        port: 4455
        timeout:
          idle: 3600s
          read: 3600s
          write: 3600s
---
# Source: ory/charts/oathkeeper/charts/oathkeeper-maester/templates/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: oathkeeper-maester-role
rules:
  - apiGroups: ["oathkeeper.ory.sh"]
    resources: ["rules"]
    verbs: ["*"]
    # TODO, fix controller call from all namespaces to single namespace
    # resourceNames:
    #   - ory-oathkeeper-rules
---
# Source: ory/charts/oathkeeper/charts/oathkeeper-maester/templates/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: oathkeeper-maester-role-binding
subjects:
  - kind: ServiceAccount
    name: oathkeeper-maester-account # Service account assigned to the controller pod.
    namespace:  default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: oathkeeper-maester-role
---
# Source: ory/charts/oathkeeper/templates/cronjob.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: ory-oathkeeper-keys-job-role
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "update", "patch"]
    resourceNames: ["release-name-oathkeeper-jwks-secret"]
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["create"]
---
# Source: ory/charts/oathkeeper/templates/cronjob.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ory-oathkeeper-keys-job-role-binding
  namespace: default
subjects:
  - kind: ServiceAccount
    name: ory-oathkeeper-keys-service-account
    namespace: default
roleRef:
  kind: Role
  name: ory-oathkeeper-keys-job-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ory/charts/hydra/templates/service-admin.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-hydra-admin
  namespace: default
  labels:
    "app.kubernetes.io/name": "hydra"
    "app.kubernetes.io/instance": "release-name"
    "app.kubernetes.io/version": "v1.11.8"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "hydra-0.23.3"
    app.kubernetes.io/component: admin
spec:
  type: ClusterIP
  ports:
    - port: 4445
      targetPort: http-admin
      protocol: TCP
      name: http
    - port: 15020
      targetPort: 15020
      protocol: TCP
      name: tcp-status-port 
  selector:
    app.kubernetes.io/name: hydra
    app.kubernetes.io/instance: release-name
---
# Source: ory/charts/hydra/templates/service-public.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-hydra-public
  namespace: default
  labels:
    "app.kubernetes.io/name": "hydra"
    "app.kubernetes.io/instance": "release-name"
    "app.kubernetes.io/version": "v1.11.8"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "hydra-0.23.3"
spec:
  type: ClusterIP
  ports:
    - port: 4444
      targetPort: http-public
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: hydra
    app.kubernetes.io/instance: release-name
---
# Source: ory/charts/oathkeeper/templates/service-api.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-oathkeeper-api
  namespace: default
  labels:
    app.kubernetes.io/component: api
    app: oathkeeper
    app.kubernetes.io/name: oathkeeper
    helm.sh/chart: oathkeeper-0.23.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.38.25-beta.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 4456
      targetPort: http-api
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: oathkeeper
    app.kubernetes.io/instance: release-name
---
# Source: ory/charts/oathkeeper/templates/service-metrics.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: oathkeeper-maester
    helm.sh/chart: oathkeeper-0.23.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.38.25-beta.1"
    app.kubernetes.io/managed-by: Helm
    mode: sidecar
  name: release-name-oathkeeper-maester-metrics
spec:
  ports:
  - name: http-metrics
    port: 8080
  selector:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/name: oathkeeper
---
# Source: ory/charts/oathkeeper/templates/service-proxy.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-oathkeeper-proxy
  namespace: default
  labels:
    app.kubernetes.io/component: proxy
    app: oathkeeper
    app.kubernetes.io/name: oathkeeper
    helm.sh/chart: oathkeeper-0.23.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.38.25-beta.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 4455
      targetPort: http-proxy
      protocol: TCP
      name: http
    # TODO!!! Custom implementation; this port is hardcoded
    - name: tcp-status-port
      port: 15020
      targetPort: 15020
      protocol: TCP
  selector:
    app.kubernetes.io/name: oathkeeper
    app.kubernetes.io/instance: release-name
---
# Source: ory/charts/postgresql/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-postgresql-headless
  labels:
    app: postgresql
    chart: postgresql-11.1.26
    release: "release-name"
    heritage: "Helm"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: postgresql
    release: "release-name"
---
# Source: ory/charts/postgresql/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-postgresql
  labels:
    app: postgresql
    chart: postgresql-11.1.26
    release: "release-name"
    heritage: "Helm"
spec:
  type: ClusterIP
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: postgresql
    release: "release-name"
    role: master
---
# Source: ory/charts/hydra/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-hydra
  namespace: default
  labels:
    "app.kubernetes.io/name": "hydra"
    "app.kubernetes.io/instance": "release-name"
    "app.kubernetes.io/version": "v1.11.8"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "hydra-0.23.3"
  annotations:
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: hydra
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 0%
    type: RollingUpdate
  template:
    metadata:
      labels:
        "app.kubernetes.io/name": "hydra"
        "app.kubernetes.io/instance": "release-name"
        "app.kubernetes.io/version": "v1.11.8"
        "app.kubernetes.io/managed-by": "Helm"
        "helm.sh/chart": "hydra-0.23.3"
      annotations:
        
        checksum/hydra-config: 1a698b05b84126dc9702229397a5aa50208f39211ad34a0023b2f38666441383
        checksum/hydra-secrets: 2849d8bcfcf43509fe56653eb21bc0e216086f906e8c7bc7985d50fc0210d447
    spec:
      volumes:
        - name: hydra-config-volume
          configMap:
            name: release-name-hydra
      serviceAccountName: default
      automountServiceAccountToken: true
      containers:
        - name: hydra
          image: "eu.gcr.io/kyma-project/external/oryd/hydra:v1.11.8"
          imagePullPolicy: IfNotPresent
          command: ["hydra"]
          volumeMounts:
            - name: hydra-config-volume
              mountPath: /etc/config
              readOnly: true
          args:
            - serve
            - all
            - --dangerous-force-http
            - --config
            - /etc/config/hydra.yaml
          ports:
            - name: http-public
              containerPort: 4444
              protocol: TCP
            - name: http-admin
              containerPort: 4445
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/alive
              port: http-admin
            failureThreshold: 5
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /health/ready
              port: http-admin
            failureThreshold: 5
            initialDelaySeconds: 30
            periodSeconds: 10
          env:
            - name: URLS_SELF_ISSUER
              value: "http://127.0.0.1:4444/"
            - name: DSN
              valueFrom:
                secretKeyRef:
                  name: release-name-hydra
                  key: dsn
            - name: SECRETS_SYSTEM
              valueFrom:
                secretKeyRef:
                  name: release-name-hydra
                  key: secretsSystem
            - name: SECRETS_COOKIE
              valueFrom:
                secretKeyRef:
                  name: release-name-hydra
                  key: secretsCookie
          resources:
            limits:
              cpu: 1000m
              memory: 400Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 100
          lifecycle:
            {}
      initContainers:
        # TODO!!! Custom implementation of the DB wait #Use only for in-cluster databases that may not be ready yet. External databases are made ready before installation.
        - name: wait-for-db
          image: "eu.gcr.io/kyma-project/external/busybox:1.34.1"
          command:
            - /bin/sh
            - -c
            - |
              # DSN is always in the form DSN=DB_TYPE://DB_USER:PASSWORD@DB_URL/DB_NAME?sslmode=disable
              # But DSN for mysql is in the form DSN=DB_TYPE://DB_USER:PASSWORD@tcp(DB_URL)/DB_NAME?parseTime=true
              
              # Extract DB_TYPE to check if the DB_TYPE is mysql
              DB_TYPE_VALUE=$(echo $DSN | awk -F '://' '{print $1}')
              
              if [ $DB_TYPE_VALUE == "mysql" ]; then
                # Extract DB_URL by cutting between @ and first / and extracting the string between brackets
                DB_URL_PORT=$(echo $DSN | cut -d '@' -f2 | cut -d '/' -f 1 | cut -d "(" -f2 | cut -d ")" -f1)
              else
                # Extract DB_URL by cutting between @ and first /
                DB_URL_PORT=$(echo $DSN | cut -d '@' -f2 | cut -d '/' -f 1 )
              fi
              
              # DB_URL is expected to be mydb.mynamespace.svc.cluster.local:1234, but the port can be optional
              # Check if it given by looking for :
              if echo "$DB_URL_PORT" | grep -q ':'; then
                # Split URL and port by :
                DB_URL=$(echo $DB_URL_PORT | cut -d ':' -f 1)
                DB_PORT=$(echo $DB_URL_PORT | cut -d ':' -f 2)
              else
                # Use the full url, since no port was given
                DB_URL=$DB_URL_PORT
              fi
              
              # If port was given, we use it, if not, empty var is expanded to 0
              until nc -zv -w 5 $DB_URL $DB_PORT; do
                echo "$DB_URL not yet ready"
                sleep 5
              done
          env:
            - name: DSN
              valueFrom:
                secretKeyRef:
                  name: release-name-hydra
                  key: dsn
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 1337 # Required for Istio CNI sidecar installation
            runAsNonRoot: true
            runAsUser: 1337 # Required for Istio CNI sidecar installation
        - name: hydra-automigrate
          image: "eu.gcr.io/kyma-project/external/oryd/hydra:v1.11.8"
          imagePullPolicy: IfNotPresent
          command: ["hydra"]
          args: ["migrate", "sql", "-e", "--yes", "--config", "/etc/config/hydra.yaml"]
          volumeMounts:
            - name: hydra-config-volume
              mountPath: /etc/config
              readOnly: true
          env:
            - name: DSN
              valueFrom:
                secretKeyRef:
                  name: release-name-hydra
                  key: dsn
          # TODO!!! Custom "securityContext"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
            - name: hydra-config-volume
              mountPath: /etc/config
              readOnly: true
---
# Source: ory/charts/oathkeeper/templates/deployment-sidecar.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-oathkeeper
  namespace: default
  labels:
    mode: sidecar
    app: oathkeeper
    app.kubernetes.io/name: oathkeeper
    helm.sh/chart: oathkeeper-0.23.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.38.25-beta.1"
    app.kubernetes.io/managed-by: Helm
spec:
# TODO!!! Custom logic that checks whether autoscaling is enabled, could exist in future Ory versions
  selector:
    matchLabels:
      app.kubernetes.io/name: oathkeeper
      app.kubernetes.io/instance: release-name
  # TODO!!! Strategy does not exist in OS charts
  strategy:
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 0%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: oathkeeper
        app.kubernetes.io/instance: release-name
      annotations:
        # important: This did not previously exist, added by OS chart        
        checksum/oathkeeper-config: f9a20a6b88256ef499a16076b729e4f376097b52fd03445c7df60d5da2021115
        checksum/oathkeeper-rules: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      volumes:
        - name: oathkeeper-config-volume
          configMap:
            name: release-name-oathkeeper-config
        - name: oathkeeper-rules-volume
          emptyDir: {}
        - name: oathkeeper-secrets-volume
          secret:
            secretName: release-name-oathkeeper
      initContainers:
        - name: init
          image: busybox:1
          volumeMounts:
            - name: oathkeeper-rules-volume
              mountPath: /etc/rules
              readOnly: false
          command:
            - sh
            - -c
            - |
              touch /etc/rules/access-rules.json
              chmod 666 /etc/rules/access-rules.json
          securityContext:
            # TODO!!! Custom implementation of security context; in OS charts it is one, in the Kyma one it is divided by sidecar, controller etc.
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
      containers:
        - name: oathkeeper
          image: "eu.gcr.io/kyma-project/external/oryd/oathkeeper:v0.38.25-beta.1"
          imagePullPolicy: IfNotPresent
          command: [ "oathkeeper", "serve", "--config", "/etc/config/config.yaml" ]
          env:
            # TODO!!! Custom implementation; OS chart only allows file, which is not ideal, should be changed in future Helm charts
            - name: MUTATORS_ID_TOKEN_CONFIG_JWKS_URL
              value: "file:///etc/secrets/jwks.json"
          volumeMounts:
            - name: oathkeeper-config-volume
              mountPath: /etc/config
              readOnly: true
            - name: oathkeeper-rules-volume
              mountPath: /etc/rules
              readOnly: true
            - name: oathkeeper-secrets-volume
              mountPath: /etc/secrets
              readOnly: true
          ports:
            - name: http-api
              containerPort: 4456
              protocol: TCP
            - name: http-proxy
              containerPort: 4455
              protocol: TCP
            # important: this might be a solution to the metrics collection of oathkeeper
            - name: http-metrics
              protocol: TCP
              containerPort: 9000
          livenessProbe:
            httpGet:
              path: /health/alive
              port: http-api
            # TODO!!! custom implementation, this(3 lines below) does not exist in OS charts; might exist in future versions
            initialDelaySeconds: 15
            periodSeconds: 10
            failureThreshold: 10
          readinessProbe:
            httpGet:
              path: /health/ready
              port: http-api
            # TODO!!! custom implementation, this(3 lines below) does not exist in OS charts; might exist in future versions
            initialDelaySeconds: 45
            periodSeconds: 10
            failureThreshold: 10
          # TODO!!! custom implementation, this(startupProbe) does not exist in OS charts; might exist in future versions
          startupProbe:
            httpGet:
              path: /health/alive
              port: http-api
            initialDelaySeconds: 45
            periodSeconds: 10
            failureThreshold: 30
          resources:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 20m
              memory: 128Mi
          # TODO!!! this is changed to adapt to the Kyma-specific oathkeeper values.yaml
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
        - name: oathkeeper-maester
          image: "eu.gcr.io/kyma-project/external/oryd/oathkeeper-maester:v0.1.5"
          imagePullPolicy: IfNotPresent
          command:
            - /manager
          args:
            - --metrics-addr=0.0.0.0:8080
            - sidecar
            - --rulesFilePath=/etc/rules/access-rules.json
          resources:
            # TODO!!! Kyma have combine the oathkeeper-maester values in the oathkeeper values, look for a way to divide them back
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 64Mi
          env:
            - name: mutatorsAvailable
              value: noop,id_token,header,cookie,hydrator
          ports:
            - containerPort: 8080
              name: metrics
          volumeMounts:
            - name: oathkeeper-rules-volume
              mountPath: /etc/rules
              readOnly: false
          # TODO!!! security context is divided in the Kyma values, but is the same in OS chart
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsGroup: 101
            runAsNonRoot: true
            runAsUser: 100
      # TODO!!! the OS chart uses maester-account
      serviceAccountName: oathkeeper-maester-account
      # TODO!!! This exists in the OS chart, but not in this one
      # automountServiceAccountToken: false
    # TODO!!! Does not exist in the OS chart, might be added later
---
# Source: ory/charts/hydra/templates/hpa.yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  namespace: default
  name: release-name-hydra
  labels:
    "app.kubernetes.io/name": "hydra"
    "app.kubernetes.io/instance": "release-name"
    "app.kubernetes.io/version": "v1.11.8"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "hydra-0.23.3"
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: release-name-hydra
  minReplicas: 1
  maxReplicas: 3
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 75
---
# Source: ory/charts/oathkeeper/templates/hpa.yaml
#Kyma resource
# TODO!!! Custom implementations - a lot of added fields, it might be possible that is due to the updated HPA apiVersion
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: release-name-oathkeeper
  namespace: default
  labels:
    app: oathkeeper
    app.kubernetes.io/name: oathkeeper
    helm.sh/chart: oathkeeper-0.23.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.38.25-beta.1"
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: release-name-oathkeeper
  minReplicas: 1
  maxReplicas: 1
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 75
---
# Source: ory/charts/postgresql/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-postgresql
  labels:
    app: postgresql
    chart: postgresql-11.1.26
    release: "release-name"
    heritage: "Helm"
spec:
  serviceName: release-name-postgresql-headless
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: postgresql
      release: "release-name"
      role: master
  template:
    metadata:
      name: release-name-postgresql
      labels:
        app: postgresql
        chart: postgresql-11.1.26
        release: "release-name"
        heritage: "Helm"
        role: master
      annotations:
        sidecar.istio.io/inject: "false"
    spec:      
      securityContext:
        fsGroup: 70
        runAsNonRoot: true
      initContainers:
        # - name: do-something
        #   image: busybox
        #   command: ['do', 'something']
        
      containers:
        - name: release-name-postgresql
          image: eu.gcr.io/kyma-project/external/postgres:11.15-alpine3.15
          imagePullPolicy: "IfNotPresent"
          resources:
            limits:
              cpu: 750m
              memory: 1024Mi
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsNonRoot: true
            runAsUser: 70
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/data/"
            - name: PGDATA
              value: "/data/pgdata"
            - name: POSTGRES_USER
              value: "hydra"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-postgresql
                  key: postgresql-password
            - name: POSTGRES_DB
              value: "db4hydra"
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "hydra" -d "db4hydra" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "hydra" -d "db4hydra" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /data/
              subPath: 
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: ory/charts/oathkeeper/templates/cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: oathkeeper-jwks-rotator
spec:
  schedule: "0 0 1 * *"
  successfulJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          initContainers:
            - name: keys-generator
              image: "eu.gcr.io/kyma-project/external/oryd/oathkeeper:v0.38.25-beta.1"
              command:
                - /bin/sh
                - -c
                - |
                  oathkeeper credentials generate --alg RS256 > /etc/secrets/jwks.json
              securityContext:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                  - ALL
                privileged: false
                runAsGroup: 101
                runAsNonRoot: true
                runAsUser: 100
              volumeMounts:
                - name: oathkeeper-keys-volume
                  mountPath: /etc/secrets
          containers:
          - name: keys-supplier
            image: eu.gcr.io/kyma-project/tpi/k8s-tools:20220525-4bd6d72e
            command:
            - /bin/bash
            - -c
            - |
              set -e
              kubectl create secret generic release-name-oathkeeper-jwks-secret \
                --from-file=/etc/secrets/jwks.json \
                -n kyma-system -o yaml --dry-run | kubectl apply -f -
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                - ALL
              privileged: false
              runAsGroup: 65534
              runAsNonRoot: true
              runAsUser: 65534
            volumeMounts:
            - name: oathkeeper-keys-volume
              mountPath: /etc/secrets
              readOnly: true
          volumes:
          - name: oathkeeper-keys-volume
            emptyDir: {}
          restartPolicy: OnFailure
          serviceAccountName: ory-oathkeeper-keys-service-account
---
# Source: ory/charts/oathkeeper/templates/cronjob.yaml
#Kyma resource
# TODO!!! Custom implementation; this job does not exist in OS
---
# Source: ory/charts/oathkeeper/templates/service-metrics.yaml
# TODO!!! Custom implementation; different labels for some reason???
---
# Source: ory/templates/db-postgresql-dr.yaml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: release-name-ory-postgresql
spec:
  host: release-name-ory-postgresql.default.svc.cluster.local
  trafficPolicy:
    tls:
      mode: DISABLE
---
# Source: ory/templates/monitoring-hydra.yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: release-name-ory-hydra-maester-metrics
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: hydra-maester
  portLevelMtls:
    8080:
      mode: PERMISSIVE
---
# Source: ory/templates/monitoring-oathkeeper.yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: release-name-ory-oathkeeper-maester-metrics
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: oathkeeper
  portLevelMtls:
    8080:
      mode: PERMISSIVE
---
# Source: ory/templates/monitoring-hydra.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    prometheus: monitoring
    app: release-name-ory
    chart: "ory-1.1.0"
    release: "release-name"
    heritage: "Helm"
  name: release-name-ory-hydra-maester
spec:
  endpoints:
  - port: http-metrics
    scheme: http
    metricRelabelings:
    - sourceLabels: [ __name__ ]
      regex: ^(go_gc_duration_seconds|go_goroutines|go_memstats_alloc_bytes|go_memstats_heap_alloc_bytes|go_memstats_heap_inuse_bytes|go_memstats_heap_sys_bytes|go_memstats_stack_inuse_bytes|go_threads|http_requests_total|process_cpu_seconds_total|process_max_fds|process_open_fds|process_resident_memory_bytes|process_start_time_seconds|process_virtual_memory_bytes|rest_client_request_latency_seconds_bucket|rest_client_requests_total|workqueue_adds_total|workqueue_depth|workqueue_queue_duration_seconds_bucket)$
      action: keep
    - sourceLabels: [__name__,le]
      regex: 'rest_client_request_latency_seconds_bucket;(0.002|0.008|0.032|0.128|0.512)' # drop buckets to reduce metric footprint
      action: drop
    - action: keep
      regex: ^rest_client_request_latency_seconds_bucket;https://.+(/api/v1.*|/apis/(apps|hydra.ory.sh).+)$ # allow metrics from core, apps and hydra API group
      sourceLabels: [__name__,url]
  namespaceSelector:
    matchNames:
      - "default"
  selector:
    matchLabels:
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: hydra-maester
---
# Source: ory/templates/monitoring-oathkeeper.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    prometheus: monitoring
    app: release-name-ory-oathkeeper-maester
    chart: "ory-1.1.0"
    release: "release-name"
    heritage: "Helm"
  name: release-name-ory-oathkeeper-maester
spec:
  endpoints:
  - port: http-metrics
    scheme: http
    metricRelabelings:
    - sourceLabels: [ __name__ ]
      regex: ^(go_gc_duration_seconds|go_goroutines|go_memstats_alloc_bytes|go_memstats_heap_alloc_bytes|go_memstats_heap_inuse_bytes|go_memstats_heap_sys_bytes|go_memstats_stack_inuse_bytes|go_threads|process_cpu_seconds_total|process_max_fds|process_open_fds|process_resident_memory_bytes|process_start_time_seconds|process_virtual_memory_bytes|rest_client_request_latency_seconds_bucket|rest_client_requests_total|workqueue_adds_total|workqueue_depth|workqueue_queue_duration_seconds_bucket)$
      action: keep
    - sourceLabels: [__name__,le]
      regex: 'rest_client_request_latency_seconds_bucket;(0.002|0.008|0.032|0.128|0.512)' # drop buckets to reduce metric footprint
      action: drop
    - sourceLabels: [__name__,url]
      regex: ^rest_client_request_latency_seconds_bucket;(https://api\..+/api/v1.*|https://api\..+/apis/(apps|oathkeeper.ory.sh).+)$ # allow metrics from core, apps and oathkeeper API group 
      action: keep
  namespaceSelector:
    matchNames:
      - "default"
  selector:
    matchLabels:
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: oathkeeper-maester
---
# Source: ory/templates/hydra-virtualservice.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: release-name-ory-hydra
  namespace: default
spec:
  gateways:
    - kyma-system/kyma-gateway
  hosts:
    - oauth2.local.kyma.dev
  http:
    - match:
        - uri:
            exact: "/oauth2/introspect"
      route:
        - destination:
            host: release-name-ory-hydra-admin
            port:
              number: 4445
    - match:
        - uri:
            prefix: "/.well-known"
        - uri:
            prefix: "/oauth2"
        - uri:
            exact: "/userinfo"
      route:
        - destination:
            host: release-name-ory-hydra-public
            port:
              number: 4444
---
# Source: ory/charts/hydra/templates/job-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-hydra-job
  labels:
    "app.kubernetes.io/name": "hydra"
    "app.kubernetes.io/instance": "release-name"
    "app.kubernetes.io/version": "v1.11.8"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "hydra-0.23.3"
  annotations:
    helm.sh/hook: pre-install, pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"
---
# Source: ory/charts/hydra/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-hydra
  namespace: default
  labels:
    "app.kubernetes.io/name": "hydra"
    "app.kubernetes.io/instance": "release-name"
    "app.kubernetes.io/version": "v1.11.8"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "hydra-0.23.3"
  annotations:
    helm.sh/hook: pre-install, pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"
    helm.sh/resource-policy: keep
type: Opaque
data:
  # Generate a random secret if the user doesn't give one. User given password has priority
  secretsSystem: "Rm9SRUhxSmxmdjBPcHVGTnV1VmZHblBidjJGZnMzMzc="
  secretsCookie: "ekNCR0JZR0VaRVpLYWlMVWtXTDZDbkNyTUlNUUZBdEI="
  dsn: "cG9zdGdyZXM6Ly9oeWRyYTpjb21wYXNzQG9yeS1wb3N0Z3Jlc3FsLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWw6NTQzMi9kYjRoeWRyYT9zc2xtb2RlPWRpc2FibGUmbWF4X2Nvbm5fbGlmZXRpbWU9MTBz"
---
# Source: ory/charts/hydra/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-hydra-test-connection"
  namespace: default
  labels:
    "app.kubernetes.io/name": "hydra"
    "app.kubernetes.io/instance": "release-name"
    "app.kubernetes.io/version": "v1.11.8"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "hydra-0.23.3"
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: healthcheck-ready
      image: busybox
      command: ['wget']
      args:  ['release-name-hydra-admin:4445/health/ready']
  restartPolicy: Never
---
# Source: ory/charts/oathkeeper/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-oathkeeper-test-connection"
  namespace: default
  labels:
    app: oathkeeper
    app.kubernetes.io/name: oathkeeper
    helm.sh/chart: oathkeeper-0.23.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.38.25-beta.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: liveness-probe
      image: "eu.gcr.io/kyma-project/external/busybox:1.34.1"
      command: ['wget']
      args:  ['http://release-name-oathkeeper-api:4456/health/alive']
    - name: readiness-probe
      image: "eu.gcr.io/kyma-project/external/busybox:1.34.1"
      command: ['wget']
      args:  ['http://release-name-oathkeeper-api:4456/health/ready']
  restartPolicy: Never
