# Global or kyma related overrides
global:
  domainName: "local.kyma.dev"
  istio:
    gateway:
      name: kyma-gateway
      namespace: kyma-system
  ory:
    oathkeeper:
      maester:
        mode: sidecar
    hydra:
      # Values used by `install-ory.sh`; if switched to true will not configure the DSN with the local database
      persistence:
        gcloud:
          enabled: false
  containerRegistry:
    path: eu.gcr.io/kyma-project
  images:
    job: bitnami/kubectl:1.25.10
# Overrides for the Hydra chart
hydra:
  maester:
    enabled: false
  ingress:
    public:
      enabled: false
    admin:
      enabled: false
  serviceMonitor:
    enabled: false
    scheme: https
    tlsConfig:
      caFile: /etc/prometheus/secrets/istio.default/root-cert.pem
      certFile: /etc/prometheus/secrets/istio.default/cert-chain.pem
      keyFile: /etc/prometheus/secrets/istio.default/key.pem
      insecureSkipVerify: true
  deployment:
    initContainerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      privileged: false
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: "50%"
        maxUnavailable: "0%"
    resources:
      limits:
        cpu: 1000m
        memory: 400Mi
      requests:
        cpu: 100m
        memory: 128Mi
    autoscaling:
      enabled: true
      minReplicas: 1
      maxReplicas: 3
      targetMemory:
        type: Utilization
        averageUtilization: 75
      targetCPU:
        type: Utilization
        averageUtilization: 80
  secret:
    enabled: true
    secretAnnotations:
      helm.sh/hook-weight: "0"
      helm.sh/hook: "pre-install"
      helm.sh/hook-delete-policy: "before-hook-creation"
      helm.sh/resource-policy: "keep"
  hydra:
    # Specifically set to 'true' to be able to serve the endpoints on 'http' as they are within Kubernetes
    dev: true
    automigration:
      enabled: true
      # Automigration is run as a Job to enabled Istio;
      # Previously it was an initContainer that started before the Istio proxy, which made the connection insecure/impossible.
      # Now the Job waits for the Istio proxy to start and then does the migration.
      type: job
      customCommand: ["/bin/sh"]
      customArgs:
        - -c
        - |
          until wget -q --spider http://127.0.0.1:15021/healthz/ready 2>/dev/null; do echo "Waiting for Istio sidecar..."; sleep 3; done;
          echo \"Sidecar available. Running...\";
          hydra migrate sql -e --yes --config /etc/config/hydra.yaml;
          x=$?; wget -q --post-data='' -S -O /dev/null http://127.0.0.1:15020/quitquitquit && exit $x
    # The ORY Hydra configuration. For a full list of available settings, check:
    # https://github.com/ory/hydra/blob/master/docs/config.yaml
    config:
      dsn: memory
      log:
        leak_sensitive_values: false
        level: trace
      serve:
        admin:
          port: 4445
        public:
          port: 4444
        tls:
          allow_termination_from:
            - 10.0.0.0/8
            - 172.16.0.0/12
            - 192.168.0.0/16
      urls:
        self: {}
# Overrides for the Oathkeeper chart
oathkeeper:
  # Any version above does not pass the liveness check as it expects at least one rule, our rules are deployed later with compass Helm chart
  # https://github.com/ory/k8s/issues/595
  image:
    tag: v0.40.1
  sidecar:
    # Previously was hardcoded by the Kyma chart
    envs:
      - name: mutatorsAvailable
        value: noop,id_token,header,cookie,hydrator
  service:
    metrics:
      # Disable metrics
      enabled: false
  # Needed to scrape metrics when using Istio
  serviceMonitor:
    scheme: https
    tlsConfig:
      caFile: /etc/prometheus/secrets/istio.default/root-cert.pem
      certFile: /etc/prometheus/secrets/istio.default/cert-chain.pem
      keyFile: /etc/prometheus/secrets/istio.default/key.pem
      insecureSkipVerify: true
  oathkeeper:
    # disabled as maester is used
    managedAccessRules: false
    config:
      authenticators:
        noop:
          enabled: true
        unauthorized:
          enabled: true
        anonymous:
          config:
            subject: anonymous
          enabled: true
        cookie_session:
          enabled: false
          config:
            # REQUIRED IF ENABLED - The session store to forward request method/path/headers to for validation
            check_session_url: https://session-store-host
            # Optionally set a list of cookie names to look for in incoming requests.
            # If unset, all requests are forwarded.
            # If set, only requests that have at least one of the set cookies will be forwarded, others will be passed to the next authenticator
            only:
              - sessionid
        oauth2_client_credentials:
          enabled: true
          config:
            # REQUIRED IF ENABLED - The OAuth 2.0 Token Endpoint that will be used to validate the client credentials.
            token_url: http://ory-stack-hydra-public.ory.svc.cluster.local:4444/oauth2/token
        oauth2_introspection:
          # Set enabled to true if the authenticator should be enabled and false to disable the authenticator. Defaults to false.
          enabled: true
          config:
            # REQUIRED IF ENABLED - The OAuth 2.0 Token Introspection endpoint.
            introspection_url: http://ory-stack-hydra-admin.ory.svc.cluster.local:4445/admin/oauth2/introspect
            # Sets the strategy to be used to validate/match the token scope. Supports "hierarchic", "exact", "wildcard", "none". Defaults
            # to "none".
            scope_strategy: exact
        # Enable the "jwt" section to allow for jwt authenticator configured.
        jwt:
          enabled: true
          config:
            jwks_urls:
              - http://ory-stack-hydra-public.ory.svc.cluster.local:4444/.well-known/jwks.json
            scope_strategy: wildcard
      authorizers:
        allow:
          enabled: true
        deny:
          enabled: true
      log:
        level: trace
      mutators:
        noop:
          enabled: true
        id_token:
          enabled: true
          config:
            # REQUIRED IF ENABLED - Sets the "iss" value of the ID Token.
            issuer_url: https://oathkeeper.local.kyma.dev/
            # REQUIRED IF ENABLED - Sets the URL where keys should be fetched from. Supports remote locations (http, https) as
            # well as local filesystem paths.
            # Keep in mind that this is created by the Oathkeeper CronJob and mounted into the pod; the name should equal to the one in the CronJob
            jwks_url: file:///etc/secrets/jwks.json
            # jwks_url: https://fetch-keys/from/this/location.json
            # jwks_url: file:///from/this/absolute/location.json
            # jwks_url: file://../from/this/relative/location.json
            # Sets the time-to-live of the ID token. Defaults to one minute. Valid time units are: s (second), m (minute), h (hour).
            ttl: 60m
        header:
          config:
            headers:
              X-Server: oathkeeper
          enabled: true
        cookie:
          enabled: true
          config:
            cookies:
              processedWith: oathkeeper
        hydrator:
          enabled: true
          config:
            api:
              url: http://compass-director.compass-system.svc.cluster.local:3000/tenant-mapping
      serve:
        proxy:
          port: 4455
        api:
          port: 4456
  secret:
    # Created by the pre-install job
    enabled: false
  deployment:
    automountServiceAccountToken: true
    livenessProbe:
      initialDelaySeconds: 15
      periodSeconds: 10
      failureThreshold: 10
    readinessProbe:
      initialDelaySeconds: 45
      periodSeconds: 10
      failureThreshold: 10
    startupProbe:
      initialDelaySeconds: 45
      periodSeconds: 10
      failureThreshold: 30
    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: "50%"
        maxUnavailable: "0%"
    resources:
      limits:
        cpu: 100m
        memory: 512Mi
      requests:
        cpu: 20m
        memory: 128Mi
    autoscaling:
      enabled: true
      maxReplicas: 3
      minReplicas: 1
      targetMemory:
        type: Utilization
        averageUtilization: 75
      targetCPU:
        type: Utilization
        averageUtilization: 80
  # Should be the same as oathkeeper.oathkeeper-maester.fullnameOverride; otherwise, the maester SA, used by the sidecar, is never found    
  maester:
    nameOverride: "oathkeeper-stack-maester"
  oathkeeper-maester:
    # Should be the same as oathkeeper.maester.nameOverride; otherwise, the maester SA, used by the sidecar, is never found
    fullnameOverride: "oathkeeper-stack-maester"
    deployment:
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 64Mi
